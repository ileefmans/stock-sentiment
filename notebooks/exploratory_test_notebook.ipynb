{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ianleefmans/Desktop/stock-sentiment\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/ianleefmans/Desktop/stock-sentiment/utils')\n",
    "from data import Database, ScrapeWSB, Stock\n",
    "from datahelper import PostDataset, CommentDataset, get_indices\n",
    "from torch.utils.data import DataLoader\n",
    "from models import FineTuneBaseModel\n",
    "%cd /Users/ianleefmans/Desktop/stock-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection established\n",
      "[(datetime.datetime(2021, 6, 6, 21, 20, 15),)]\n",
      "[('COMMENTS',), ('POSTS',), ('STOCKS',)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db = Database()\n",
    "db.use_database('DB1')\n",
    "\n",
    "print(db.query('show tables;'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nnpn9z',), ('nns6mk',), ('nn4aej',), ('nmpk6k',), ('nmjo2l',), ('nmabst',), ('nm53o9',), ('nmthag',), ('nmpq6d',)]\n"
     ]
    }
   ],
   "source": [
    "print(db.query(\"SELECT POST_ID FROM POSTS WHERE TARGET IN (0, 1)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"I'd buy GME for a dollar!\", -1),\n",
       " ('First ever gains post.. GME and AMC to the moon we hodl 🦍💎🙌🚀🌝', -1),\n",
       " ('What if I told you there was a profitable company with a bright future that had short interest highe',\n",
       "  -1),\n",
       " ('Relative Rotation: GME, AMC, BB, NOK', -1),\n",
       " ('(Part 2) AMC & GME Price Action Comparison before their squeeze', -1),\n",
       " ('The Gang Buys $GME', -1),\n",
       " ('We like the stock $AMC $GME', -1),\n",
       " ('Out of GME and into BB. I think much more upside there and social mentions of BB through the roof, w',\n",
       "  -1),\n",
       " ('Fresh out of the womb tube here gentlemen! I missed GME. I missed AMC. Putting my toes in the water',\n",
       "  -1),\n",
       " ('Another $GME, $AMC, $BB aimed for the MOON', -1),\n",
       " ('Is $SNDL going to do the amc/gme? 🚀', -1),\n",
       " ('Lets Go!!! GME to the moon!!!', -1),\n",
       " ('GME to the moon!!! And sometimes bananas fuck assholes!!! 💎🙌🦍', -1),\n",
       " (\"Current Price Action of GME is shockingly similar to the past two major run-ups and 'micro squeezes'\",\n",
       "  -1),\n",
       " ('Ape wonders about AMC,GME and BB advertising budget 🙉🚀 SpaceX', -1),\n",
       " (\"Added $165K more to the GME July 800s YOLO - $646K at risk let's GOOOOO\",\n",
       "  -1),\n",
       " ('The day I became a retard. Bought GME pre market at 400+ the day the stocks cratered. Sold Nvidia to',\n",
       "  -1),\n",
       " ('(Part 3) AMC & GME Price Action Comparison before their squeeze', -1),\n",
       " ('❗“The Monkey Fool”❗ 🤡: A Parody on How Bullsh!t FUD Campaigns Are Made for GameStop 😹 A Professional',\n",
       "  -1),\n",
       " ('BB | AMC | GME - Daily Popular Ticker Thread for June 06, 2021', -1),\n",
       " ('$AMC and $GME Are Going To Cycle through Dips And Peaks This Year.', -1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query('''SELECT TITLE, TARGET FROM POSTS;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query('''SELECT COMMENT, TARGET FROM COMMENTS;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 1),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 1),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 1),)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query(\"SELECT LAST_SCRAPED FROM POSTS WHERE LAST_SCRAPED >= DATE_SUB((SELECT LAST_SCRAPED FROM STOCKS WHERE STOCK_ID = 'GME'),INTERVAL 6 HOUR);\")\n",
    "         \n",
    "         \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 2),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 1),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 1),),\n",
       " (datetime.datetime(2021, 6, 6, 17, 24, 1),)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query(\"SELECT LAST_SCRAPED FROM POSTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = db.query(\"SELECT POST_ID FROM POSTS WHERE StOCK_ID='{}';\".format(\"GME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection established\n",
      "[(datetime.datetime(2021, 6, 6, 17, 23, 59),)]\n",
      "connection established\n",
      "[(datetime.datetime(2021, 6, 6, 17, 24),)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection established\n",
      "[(datetime.datetime(2021, 6, 6, 17, 24, 3),)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:08<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "db = Database()\n",
    "db.use_database('DB1')\n",
    "#db.drop_table(\"LABELED_POSTS\")\n",
    "#db.drop_table(\"LABELED_COMMENTS\")\n",
    "\n",
    "db.initialize_tables()\n",
    "\n",
    "scrapewsb = ScrapeWSB('GME', 11, 2)\n",
    "\n",
    "df = scrapewsb.scrape()\n",
    "scrapewsb.convert(df)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection established\n",
      "[(datetime.datetime(2021, 5, 30, 0, 20, 6),)]\n"
     ]
    }
   ],
   "source": [
    "db = Database()\n",
    "db.use_database('DB1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.drop_table('COMMENTS')\n",
    "db.drop_table(\"POSTS\")\n",
    "db.drop_table(\"STOCKS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = get_indices(\"GME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection established\n",
      "[(datetime.datetime(2021, 6, 2, 17, 22, 24),)]\n"
     ]
    }
   ],
   "source": [
    "post_train = PostDataset(512, indices['post_train'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_trainloader = DataLoader(\n",
    "    dataset=post_train, \n",
    "    batch_size=3, \n",
    "    num_workers=0,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/ianleefmans/opt/anaconda3/envs/StockSentiment/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "sample = iter(post_trainloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'post': ['GME & AMC',\n",
       "  'What if GME does a OSTK.',\n",
       "  'Proper 401k weighting for a 26 year old? 99.8% $GME, 0.2% $AMC'],\n",
       " 'post_input_ids': tensor([[  101, 14748,  2036,  ...,     0,     0,     0],\n",
       "         [  101,  1327,  1191,  ...,     0,     0,     0],\n",
       "         [  101,  5096,  3365,  ...,     0,     0,     0]]),\n",
       " 'post_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'target': tensor([[[0., 1.]],\n",
       " \n",
       "         [[1., 0.]],\n",
       " \n",
       "         [[0., 1.]]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[-0.6015,  0.3814,  0.9997,  ...,  0.9999, -0.6898,  0.9579],\n",
      "        [-0.6080,  0.4887,  0.9999,  ...,  1.0000, -0.9328,  0.9946],\n",
      "        [-0.5415,  0.3671,  0.9996,  ...,  0.9998, -0.7365,  0.9712]],\n",
      "       grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = FineTuneBaseModel()\n",
    "\n",
    "out = model(input_ids=sample['post_input_ids'], attention_masks=sample['post_attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7030, 0.2970],\n",
       "        [0.7028, 0.2972],\n",
       "        [0.7087, 0.2913]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
